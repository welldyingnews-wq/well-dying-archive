name: Well-Dying News Scraper

on:
  push:
  schedule:
    - cron: '*/30 * * * *' # 30분마다
  workflow_dispatch:

jobs:
  run-scraper:
    runs-on: ubuntu-latest

    steps:
    - name: 1. 코드 내려받기
      uses: actions/checkout@v3

    - name: 2. 파이썬 설치
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: 3. 라이브러리 설치
      run: pip install -r requirements.txt

    - name: 4. 구글 인증 파일 생성
      # 깃허브 Secret 내용을 파일로 만듭니다.
      run: |
        echo '${{ secrets.GOOGLE_SHEET_JSON }}' > service_account.json

    - name: 5. 수집기 실행
      env:
        # [필수 1] Supabase (저장소)
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        
        # [필수 2] 구글 시트 (설정 읽기용)
        GOOGLE_SHEET_JSON: ${{ secrets.GOOGLE_SHEET_JSON }}
        GOOGLE_SHEET_JSON_PATH: service_account.json
        
        # [필수 3] 네이버 검색
        NAVER_CLIENT_ID: ${{ secrets.NAVER_CLIENT_ID }}
        NAVER_CLIENT_SECRET: ${{ secrets.NAVER_CLIENT_SECRET }}
        
        # [선택] 미래를 위해 미리 넣어두는 키들 (지금 안 써도 에러 안 남)
        GENAI_API_KEY: ${{ secrets.GENAI_API_KEY }}
        NEWS_API_KEY: ${{ secrets.NEWS_API_KEY }}
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        
      run: python collector.py
