name: Well-Dying News Scraper

on:
  push:
  schedule:
    - cron: '*/30 * * * *'
  workflow_dispatch:

jobs:
  run-scraper:
    runs-on: ubuntu-latest

    steps:
    - name: 1. 코드 내려받기
      uses: actions/checkout@v3

    - name: 2. 파이썬 설치
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: 3. 라이브러리 설치
      run: pip install -r requirements.txt

    - name: 4. 구글 인증 파일 생성
      # ⭐ [수정됨] 깃허브 Secret 이름(GOOGLE_SHEET_JSON)과 똑같이 맞춰야 합니다!
      run: |
        echo '${{ secrets.GOOGLE_SHEET_JSON }}' > service_account.json

    - name: 5. 수집기 실행
      env:
        # Supabase 열쇠
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        
        # 구글 시트 열쇠 (위에서 파일을 만들었으니 경로는 service_account.json)
        GOOGLE_SHEET_JSON: ${{ secrets.GOOGLE_SHEET_JSON }}
        GOOGLE_SHEET_JSON_PATH: service_account.json
        
        # 네이버 열쇠
        NAVER_CLIENT_ID: ${{ secrets.NAVER_CLIENT_ID }}
        NAVER_CLIENT_SECRET: ${{ secrets.NAVER_CLIENT_SECRET }}
        
        # 기타 키들
        GENAI_API_KEY: ${{ secrets.GENAI_API_KEY }}
        NEWS_API_KEY: ${{ secrets.NEWS_API_KEY }}
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        
      run: python collector.py
