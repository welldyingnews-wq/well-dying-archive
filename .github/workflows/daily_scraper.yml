name: Well-Dying News Scraper

on:
  schedule:
    # 매일 한국시간 아침 9시, 오후 6시 실행 (UTC 기준 0시, 9시)
    - cron: '0 0,9 * * *'
  workflow_dispatch: # 깃허브 웹사이트에서 버튼 눌러서 수동 실행 가능

jobs:
  run-scraper:
    runs-on: ubuntu-latest

    steps:
    - name: 1. 코드 내려받기
      uses: actions/checkout@v3

    - name: 2. 파이썬 설치
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: 3. 라이브러리 설치
      run: pip install -r requirements.txt

    - name: 4. 비밀 열쇠 파일 생성 (가장 중요!)
      # 깃허브 비밀금고에 저장된 텍스트를 꺼내서 실제 파일(service_account.json)로 만듭니다.
      run: |
        echo '${{ secrets.GOOGLE_JSON_CONTENT }}' > service_account.json

    - name: 5. 수집기 실행
      env:
        # 비밀금고(Secrets)에서 꺼내와서 환경변수로 넣어줍니다.
        GENAI_API_KEY: ${{ secrets.GENAI_API_KEY }}
        NAVER_CLIENT_ID: ${{ secrets.NAVER_CLIENT_ID }}
        NAVER_CLIENT_SECRET: ${{ secrets.NAVER_CLIENT_SECRET }}
        NEWS_API_KEY: ${{ secrets.NEWS_API_KEY }}
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        GOOGLE_SHEET_JSON_PATH: service_account.json
      run: python collector.py
