import os
import json
import feedparser
import urllib.parse
import requests
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from datetime import datetime
from dotenv import load_dotenv

# â­ Supabase ì €ì¥ ë‹´ë‹¹ (database.py íŒŒì¼ì´ ê°™ì€ í´ë”ì— ìˆì–´ì•¼ í•¨)
import database 

load_dotenv()

# ==========================================
# 1. êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° (ì„¤ì •ê°’ ì½ê¸° ì „ìš©)
# ==========================================
def get_sheet_client():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    json_filename = "service_account.json"

    # ê¹ƒí—ˆë¸Œ ì„œë²„ì—ëŠ” íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ, í™˜ê²½ë³€ìˆ˜(Secret)ì—ì„œ êº¼ë‚´ì„œ ë§Œë“¦
    if not os.path.exists(json_filename):
        json_content = os.getenv("GOOGLE_SHEET_JSON")
        if not json_content:
            print("âŒ ì—ëŸ¬: êµ¬ê¸€ ì‹œíŠ¸ í‚¤(GOOGLE_SHEET_JSON)ê°€ ê¹ƒí—ˆë¸Œ Secretsì— ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        with open(json_filename, "w") as f:
            f.write(json_content)
        print("âœ… ê¹ƒí—ˆë¸Œ Secretì„ ì´ìš©í•´ ì¸ì¦ íŒŒì¼ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

    creds = ServiceAccountCredentials.from_json_keyfile_name(json_filename, scope)
    return gspread.authorize(creds)

def load_configs(client):
    """êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ì„¤ì •ê°’(í‚¤ì›Œë“œ, ê¸ˆì§€ì–´ ë“±)ë§Œ ì™ ë¹¼ì˜¤ê¸°"""
    print("ğŸ“‹ êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ì„¤ì •ì„ ì½ì–´ì˜µë‹ˆë‹¤...")
    wb = client.open("Global Well-Dying Archive")
    
    # 1. íƒ€ê²Ÿ êµ­ê°€
    targets = []
    try:
        for r in wb.worksheet("Config").get_all_records():
            if r.get('êµ­ê°€ì½”ë“œ'): targets.append(r)
    except: targets = [{'code': 'US', 'lang': 'en', 'name': 'ë¯¸êµ­'}]

    # 2. í‚¤ì›Œë“œ
    keywords = []
    try:
        for r in wb.worksheet("Keywords").get_all_records():
            if r.get('í‚¤ì›Œë“œ'): keywords.append(r['í‚¤ì›Œë“œ'])
    except: keywords = ["Euthanasia"]

    # 3. RSS ì‚¬ì´íŠ¸
    sites = []
    try:
        for r in wb.worksheet("Sites").get_all_records():
            if r.get('RSSì£¼ì†Œ'): sites.append({'name': r['ì‚¬ì´íŠ¸ëª…'], 'url': r['RSSì£¼ì†Œ']})
    except: sites = []

    # 4. ê¸ˆì§€ì–´
    ban_words = []
    try:
        for r in wb.worksheet("BanWords").get_all_records():
            if r.get('ê¸ˆì§€ì–´'): ban_words.append(r['ê¸ˆì§€ì–´'])
    except: pass
    
    return targets, keywords, sites, ban_words

# ==========================================
# 2. ë‰´ìŠ¤ ìˆ˜ì§‘ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
# ==========================================
def is_junk(title, ban_words):
    for bad_word in ban_words:
        if bad_word.lower() in title.lower(): return True
    return False

def fetch_google_news_direct(keywords, targets, ban_words):
    results = []
    base_url = "https://news.google.com/rss/search"
    for target in targets:
        for kw in keywords:
            try:
                search_kw = kw
                if target['code'] == 'JP' and kw == 'Euthanasia': search_kw = 'å®‰æ¥½æ­»'
                params = {"q": search_kw, "hl": target['lang'], "gl": target['code'], "ceid": f"{target['code']}:{target['lang']}"}
                feed = feedparser.parse(f"{base_url}?{urllib.parse.urlencode(params)}")
                for entry in feed.entries[:2]:
                    if not is_junk(entry.title, ban_words):
                        results.append({'title': entry.title, 'link': entry.link, 'source_type': f"Google({target['name']})"})
            except: pass
    return results

def fetch_rss_sites(sites, ban_words):
    results = []
    for site in sites:
        try:
            feed = feedparser.parse(site['url'])
            for entry in feed.entries[:3]:
                if not is_junk(entry.title, ban_words):
                    results.append({'title': entry.title, 'link': entry.link, 'source_type': f"Blog({site['name']})"})
        except: pass
    return results

def fetch_naver_news(keywords, ban_words):
    results = []
    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")
    if not client_id: return []
    
    headers = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}
    for kw in keywords[:5]:
        try:
            url = f"https://openapi.naver.com/v1/search/news.json?query={kw}&display=3&sort=sim"
            res = requests.get(url, headers=headers).json()
            for item in res.get('items', []):
                title = item['title'].replace('<b>','').replace('</b>','').replace('&quot;', '"')
                if not is_junk(title, ban_words):
                    results.append({'title': title, 'link': item['link'], 'source_type': 'NAVER(êµ­ë‚´)'})
        except: pass
    return results

# ==========================================
# 3. ë©”ì¸ ì‹¤í–‰ (ìˆ˜ì§‘ -> Supabase ì €ì¥)
# ==========================================
def main():
    print("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘ê¸° ê°€ë™ (ì„¤ì •:êµ¬ê¸€ì‹œíŠ¸ / ì €ì¥:Supabase)")
    
    # 1. êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²°
    client = get_sheet_client()
    if not client: return

    # 2. ì„¤ì •ê°’ ë¡œë“œ
    targets, keywords, sites, ban_words = load_configs(client)
    print(f"ğŸ” í‚¤ì›Œë“œ: {keywords}")
    print(f"ğŸš« ê¸ˆì§€ì–´: {len(ban_words)}ê°œ ì ìš©ë¨")

    # 3. ë‰´ìŠ¤ ìˆ˜ì§‘
    all_news = []
    all_news.extend(fetch_naver_news(keywords, ban_words))
    all_news.extend(fetch_google_news_direct(keywords, targets, ban_words))
    all_news.extend(fetch_rss_sites(sites, ban_words))
    
    print(f"ğŸ“¦ ì´ {len(all_news)}ê°œ ê¸°ì‚¬ í™•ë³´.")

    # 4. ë‚ ì§œ ì°ê³  Supabaseì— ì €ì¥
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M")
    for news in all_news:
        news['collected_at'] = current_time

    if all_news:
        try:
            # â­ ì—¬ê¸°ê°€ í•µì‹¬: ì‹œíŠ¸ ì €ì¥ ì½”ë“œëŠ” ì—†ê³ , DB ì €ì¥ ì½”ë“œë§Œ ìˆìŒ!
            count = database.save_news(all_news)
            print(f"ğŸ’¾ Supabase ì €ì¥ ì™„ë£Œ: {count}ê±´ (ì¤‘ë³µ ì œì™¸)")
        except Exception as e:
            print(f"ğŸ”¥ ì €ì¥ ì‹¤íŒ¨: {e}")
            print("Hint: database.py íŒŒì¼ì´ ìˆëŠ”ì§€, Supabase URL/KEYê°€ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    else:
        print("â˜ï¸ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
