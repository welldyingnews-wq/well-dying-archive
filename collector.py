import os
import time
import json
import requests
import feedparser
import urllib.parse
from datetime import datetime, timedelta
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from dotenv import load_dotenv

load_dotenv()

# ==========================================
# 0. ê¸ˆì§€ì–´ ëª©ë¡
# ==========================================
EXCLUDE_KEYWORDS = ["ê²Œì„", "Game", "ì£¼ì‹", "ì¦ì‹œ", "ì¢…ëª©", "ì˜í™”", "Movie", "ë“œë¼ë§ˆ", "ì›¹íˆ°", "ë¦¬ë·°", "ì´ë²¤íŠ¸"]

# ==========================================
# 1. ì„¤ì • ë° ì´ˆê¸°í™”
# ==========================================
def get_sheet_client():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    json_path = os.getenv("GOOGLE_SHEET_JSON_PATH", "service_account.json")
    creds = ServiceAccountCredentials.from_json_keyfile_name(json_path, scope)
    return gspread.authorize(creds)

def check_time_and_run(client):
    """ì§€ì •ëœ ì‹œê°„ì´ ì§€ë‚¬ëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    try:
        sh = client.open("Global Well-Dying Archive").worksheet("Settings")
        
        # ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°
        interval = int(sh.cell(2, 2).value) # B2: ìˆ˜ì§‘ì£¼ê¸°(ë¶„)
        last_run_str = sh.cell(3, 2).value  # B3: ë§ˆì§€ë§‰ ì‹¤í–‰ì‹œê°„
        
        last_run = datetime.strptime(last_run_str, "%Y-%m-%d %H:%M:%S")
        time_diff = datetime.now() - last_run
        minutes_passed = time_diff.total_seconds() / 60
        
        print(f"â° ì„¤ì • ì£¼ê¸°: {interval}ë¶„ | ì§€ë‚œ ì‹œê°„: {int(minutes_passed)}ë¶„")
        
        if minutes_passed < interval:
            print("ğŸ’¤ ì•„ì§ ì¼í•  ì‹œê°„ì´ ì•„ë‹™ë‹ˆë‹¤. ë‹¤ì‹œ ì¡ë‹ˆë‹¤.")
            return False # ì‹¤í–‰í•˜ì§€ ë§ˆ!
        
        # ì‹¤í–‰í•˜ê¸°ë¡œ ê²°ì •í–ˆìœ¼ë©´, ì§€ê¸ˆ ì‹œê°„ì„ ê¸°ë¡
        sh.update_cell(3, 2, datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        return True # ì‹¤í–‰í•´!
        
    except Exception as e:
        print(f"âš ï¸ ì‹œê°„ ì„¤ì • í™•ì¸ ì¤‘ ì˜¤ë¥˜ (ê·¸ëƒ¥ ì‹¤í–‰í•©ë‹ˆë‹¤): {e}")
        return True

def load_configs(client):
    wb = client.open("Global Well-Dying Archive")
    
    # 1. êµ­ê°€, 2. í‚¤ì›Œë“œ, 3. ì‚¬ì´íŠ¸ ë¡œë“œ (ê¸°ì¡´ê³¼ ë™ì¼)
    targets = []
    try:
        for r in wb.worksheet("Config").get_all_records():
            if r.get('êµ­ê°€ì½”ë“œ'): targets.append({'code': r['êµ­ê°€ì½”ë“œ'], 'lang': r['ì–¸ì–´'], 'name': r['êµ­ê°€ëª…']})
    except: targets = [{'code': 'US', 'lang': 'en', 'name': 'ë¯¸êµ­(ê¸°ë³¸)'}]

    keywords = []
    try:
        for r in wb.worksheet("Keywords").get_all_records():
            if r.get('í‚¤ì›Œë“œ'): keywords.append(r['í‚¤ì›Œë“œ'])
    except: keywords = ["Euthanasia"]

    sites = []
    try:
        for r in wb.worksheet("Sites").get_all_records():
            if r.get('RSSì£¼ì†Œ'): sites.append({'name': r['ì‚¬ì´íŠ¸ëª…'], 'url': r['RSSì£¼ì†Œ']})
    except: sites = []

    ban_words = []
    try:
        for r in wb.worksheet("BanWords").get_all_records():
            if r.get('ê¸ˆì§€ì–´'): ban_words.append(r['ê¸ˆì§€ì–´'])
    except: ban_words = EXCLUDE_KEYWORDS

    return targets, keywords, sites, ban_words

# ==========================================
# 2. í•„í„°ë§ ë° ìˆ˜ì§‘ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼)
# ==========================================
def is_junk(title, ban_words):
    for bad_word in ban_words:
        if bad_word.lower() in title.lower(): return True
    return False

def fetch_google_news_direct(keywords, targets, ban_words):
    results = []
    base_url = "https://news.google.com/rss/search"
    for target in targets:
        for kw in keywords:
            try:
                search_kw = kw
                if target['code'] == 'JP' and kw == 'Euthanasia': search_kw = 'å®‰æ¥½æ­»'
                params = {"q": search_kw, "hl": target['lang'], "gl": target['code'], "ceid": f"{target['code']}:{target['lang']}"}
                feed = feedparser.parse(f"{base_url}?{urllib.parse.urlencode(params)}")
                for entry in feed.entries[:2]:
                    if not is_junk(entry.title, ban_words):
                        results.append({'title': entry.title, 'link': entry.link, 'source_type': f"Google({target['name']})"})
            except: pass
    return results

def fetch_rss_sites(sites, ban_words):
    results = []
    for site in sites:
        try:
            feed = feedparser.parse(site['url'])
            for entry in feed.entries[:3]:
                if not is_junk(entry.title, ban_words):
                    results.append({'title': entry.title, 'link': entry.link, 'source_type': f"Blog({site['name']})"})
        except: pass
    return results

def fetch_naver_news(keywords, ban_words):
    results = []
    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")
    if not client_id or not client_secret: return []
    headers = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}
    for kw in keywords[:5]:
        try:
            res = requests.get(f"https://openapi.naver.com/v1/search/news.json?query={kw}&display=3&sort=sim", headers=headers).json()
            for item in res.get('items', []):
                title = item['title'].replace('<b>','').replace('</b>','')
                if not is_junk(title, ban_words):
                    results.append({'title': title, 'link': item['link'], 'source_type': 'NAVER(êµ­ë‚´)'})
        except: pass
    return results

# ==========================================
# 3. ë©”ì¸ ì‹¤í–‰ (ì‹œê°„ ì²´í¬ ë¡œì§ ì¶”ê°€ë¨)
# ==========================================
def main():
    print("ğŸš€ ìŠ¤ë§ˆíŠ¸ ìˆ˜ì§‘ê¸° ê°€ë™ ì¤‘...")
    client = get_sheet_client()
    
    # â­ ì—¬ê¸°ê°€ í•µì‹¬! (ì‹œê°„ì´ ì•ˆ ëìœ¼ë©´ ì—¬ê¸°ì„œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ)
    if not check_time_and_run(client):
        return 

    targets, keywords, sites, ban_words = load_configs(client)
    
    all_news = []
    all_news.extend(fetch_naver_news(keywords, ban_words))
    all_news.extend(fetch_google_news_direct(keywords, targets, ban_words))
    all_news.extend(fetch_rss_sites(sites, ban_words))
    
    print(f"ğŸ“¦ {len(all_news)}ê°œ ê¸°ì‚¬ í™•ë³´. ì €ì¥ ì‹œì‘...")
    
    sheet = client.open("Global Well-Dying Archive").worksheet("News")
    existing_links = sheet.col_values(8)
    
    new_rows = []
    for news in all_news:
        if news['link'] in existing_links: continue
        translate_formula = f'=GOOGLETRANSLATE("{news["title"]}", "auto", "ko")'
        new_rows.append([datetime.now().strftime("%Y-%m-%d %H:%M"), news['source_type'], "ìˆ˜ì§‘ë¨", news['title'], translate_formula, "", "", news['link']])

    if new_rows:
        sheet.append_rows(new_rows, value_input_option='USER_ENTERED')
        print(f"ğŸ’¾ {len(new_rows)}ê°œ ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ!")
    else:
        print("â˜ï¸ ìƒˆë¡œ ì—…ë°ì´íŠ¸ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
