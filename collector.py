import os
import time
import json
import requests
import feedparser
import urllib.parse
import pandas as pd
from datetime import datetime
import google.generativeai as genai
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from dotenv import load_dotenv

load_dotenv()

# ==========================================
# 1. ì„¤ì • ë° ì´ˆê¸°í™”
# ==========================================
def get_sheet_client():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    json_path = os.getenv("GOOGLE_SHEET_JSON_PATH", "service_account.json")
    creds = ServiceAccountCredentials.from_json_keyfile_name(json_path, scope)
    return gspread.authorize(creds)

def configure_genai():
    api_key = os.getenv("GENAI_API_KEY")
    genai.configure(api_key=api_key)
    return genai.GenerativeModel('models/gemini-1.5-flash')

def load_configs(client):
    wb = client.open("Global Well-Dying Archive")
    
    targets = []
    try:
        for r in wb.worksheet("Config").get_all_records():
            if r.get('êµ­ê°€ì½”ë“œ'): targets.append({'code': r['êµ­ê°€ì½”ë“œ'], 'lang': r['ì–¸ì–´'], 'name': r['êµ­ê°€ëª…']})
    except: targets = [{'code': 'US', 'lang': 'en', 'name': 'ë¯¸êµ­(ê¸°ë³¸)'}]

    keywords = []
    try:
        for r in wb.worksheet("Keywords").get_all_records():
            if r.get('í‚¤ì›Œë“œ'): keywords.append(r['í‚¤ì›Œë“œ'])
    except: keywords = ["Euthanasia"]

    sites = []
    try:
        for r in wb.worksheet("Sites").get_all_records():
            if r.get('RSSì£¼ì†Œ'): sites.append({'name': r['ì‚¬ì´íŠ¸ëª…'], 'url': r['RSSì£¼ì†Œ']})
    except: sites = []

    return targets, keywords, sites

# ==========================================
# 2. ìˆ˜ì§‘ê¸° (ì§ì ‘ í†µì‹  ë²„ì „)
# ==========================================
def fetch_google_news_direct(keywords, targets):
    results = []
    base_url = "https://news.google.com/rss/search"
    
    for target in targets:
        print(f"  âœˆï¸ {target['name']} ë‰´ìŠ¤ íƒìƒ‰ ì¤‘...")
        for kw in keywords:
            try:
                # 1. ê²€ìƒ‰ì–´ êµ­ê°€ë³„ ìµœì í™”
                search_kw = kw
                if target['code'] == 'JP' and kw == 'Euthanasia': search_kw = 'å®‰æ¥½æ­»'
                
                # 2. êµ¬ê¸€ ë‰´ìŠ¤ RSS ì£¼ì†Œ ì§ì ‘ ìƒì„± (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì œê±°ë¨)
                params = {
                    "q": search_kw,
                    "hl": target['lang'],
                    "gl": target['code'],
                    "ceid": f"{target['code']}:{target['lang']}"
                }
                query_string = urllib.parse.urlencode(params)
                rss_url = f"{base_url}?{query_string}"
                
                # 3. RSS íŒŒì‹±
                feed = feedparser.parse(rss_url)
                
                for entry in feed.entries[:2]:
                    results.append({
                        'title': entry.title,
                        'link': entry.link,
                        'content': entry.title,
                        'source_type': f"Google({target['name']})"
                    })
            except Exception as e:
                print(f"    âš ï¸ {target['name']} ì—ëŸ¬: {e}")
    return results

def fetch_rss_sites(sites):
    results = []
    for site in sites:
        print(f"  ğŸ“¡ {site['name']} ë¸”ë¡œê·¸ íƒìƒ‰ ì¤‘...")
        try:
            feed = feedparser.parse(site['url'])
            for entry in feed.entries[:3]:
                results.append({
                    'title': entry.title,
                    'link': entry.link,
                    'content': getattr(entry, 'summary', entry.title),
                    'source_type': f"Blog({site['name']})"
                })
        except Exception as e:
            print(f"    âš ï¸ {site['name']} ì—ëŸ¬: {e}")
    return results

def fetch_naver_news(keywords):
    results = []
    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")
    
    if not client_id or not client_secret:
        return []

    headers = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}
    for kw in ["ì›°ë‹¤ì‰", "ì¡´ì—„ì‚¬", "í˜¸ìŠ¤í”¼ìŠ¤"]:
        try:
            url = f"https://openapi.naver.com/v1/search/news.json?query={kw}&display=3&sort=sim"
            res = requests.get(url, headers=headers).json()
            for item in res.get('items', []):
                results.append({
                    'title': item['title'].replace('<b>','').replace('</b>',''),
                    'link': item['link'],
                    'content': item['description'],
                    'source_type': 'NAVER(êµ­ë‚´)'
                })
        except: pass
    return results

# ==========================================
# 3. AI ë¶„ì„ê¸°
# ==========================================
def analyze_news(model, news):
    prompt = f"""
    ë‹¹ì‹ ì€ ì›°ë‹¤ì‰ ë‰´ìŠ¤ í¸ì§‘ìì…ë‹ˆë‹¤. ì´ ê¸°ì‚¬ê°€ 'ì£½ìŒ, í˜¸ìŠ¤í”¼ìŠ¤, ì¥ë¡€, ì¡´ì—„ì‚¬'ì™€ ê´€ë ¨ ìˆëŠ”ì§€ ë¶„ì„í•˜ì„¸ìš”.
    ì™¸êµ­ì–´ë¼ë©´ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì„œ ìš”ì•½í•˜ì„¸ìš”.

    ì œëª©: {news['title']}
    ë‚´ìš©: {news['content']}

    [ì‘ë‹µ í˜•ì‹ JSON]
    {{
        "is_relevant": true/false,
        "category": "ì •ì±…/ê¸°ìˆ /ë¬¸í™”/ì‚¬ê±´ ì¤‘ íƒ1",
        "summary": "3ë¬¸ì¥ ì´ë‚´ í•œêµ­ì–´ ìš”ì•½",
        "sentiment": "í¬ë§/ë…¼ìŸ/ë¹„ë³´ ì¤‘ íƒ1",
        "priority": 1~5 (ì ìˆ˜)
    }}
    """
    try:
        res = model.generate_content(prompt)
        text = res.text.replace('```json','').replace('```','').strip()
        return json.loads(text)
    except: return None

# ==========================================
# 4. ë©”ì¸ ì‹¤í–‰
# ==========================================
def main():
    print("ğŸš€ ì‹œìŠ¤í…œ ê°€ë™ ì‹œì‘...")
    client = get_sheet_client()
    targets, keywords, sites = load_configs(client)
    model = configure_genai()
    
    all_news = []
    all_news.extend(fetch_naver_news(keywords))
    # ë³€ê²½ëœ í•¨ìˆ˜ ì‚¬ìš©
    all_news.extend(fetch_google_news_direct(keywords, targets))
    all_news.extend(fetch_rss_sites(sites))
    
    print(f"ğŸ“¦ ì´ {len(all_news)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘. AI ë¶„ì„ ì‹œì‘...")
    
    sheet = client.open("Global Well-Dying Archive").worksheet("News")
    existing_links = sheet.col_values(8)
    
    new_rows = []
    for i, news in enumerate(all_news):
        if news['link'] in existing_links: continue
        
        print(f"[{i+1}/{len(all_news)}] â³ AI ë¶„ì„ ì¤‘... (15ì´ˆ ëŒ€ê¸°) - {news['title'][:10]}")
        time.sleep(15) 
        
        analysis = analyze_news(model, news)
        
        if analysis and analysis['is_relevant']:
            row = [
                datetime.now().strftime("%Y-%m-%d %H:%M"),
                news['source_type'],
                analysis['category'],
                news['title'],
                analysis['summary'],
                analysis['sentiment'],
                analysis['priority'],
                news['link']
            ]
            new_rows.append(row)
            print(f"  âœ… ì €ì¥ ëŒ€ê¸°: {news['title'][:15]}...")
        else:
            print("  âŒ ê´€ë ¨ ì—†ìŒ/ë¶„ì„ ì‹¤íŒ¨")

    if new_rows:
        sheet.append_rows(new_rows)
        print(f"ğŸ’¾ {len(new_rows)}ê°œ ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ!")
    else:
        print("â˜ï¸ ì €ì¥í•  ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
